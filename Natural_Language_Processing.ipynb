{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Natural_Language_Processing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1Q-EeoMKf4CYQD6m1FJv6ZGz0uMthBNG8",
      "authorship_tag": "ABX9TyPjQSDpzibtxNb7rK0RMqxM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumar-mahendra/66DaysOfData/blob/main/Natural_Language_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C-luwiHREbB"
      },
      "source": [
        "# Introduction to NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jO0ExFcRNTL"
      },
      "source": [
        "## Word Tokenization, padding, Removing Stopwords and Cleaning Data\r\n",
        "\r\n",
        "#### Dated : 26-01-2021"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQ9TISPNQYpm"
      },
      "source": [
        "\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGRsiYRjQ1vQ",
        "outputId": "def426ec-c35d-4501-d63f-4b4f439da00a"
      },
      "source": [
        "\r\n",
        "tokenizer = Tokenizer(num_words=100,oov_token=\"<oov>\")  #oov means out_of_vocabulory text which will replace any irrelevent word or word which is new or not known to \r\n",
        "                                                        # model as \"<oov>\"(here you can take any string but make sure its not in your sequence)\r\n",
        "\r\n",
        "#sample input , a list of sentences \r\n",
        "sentences = ['I am Mahendra Kumar!!!', 'I am currently in my second year of study,at \"IIT Goa\".','My branch is maths & computing','Want to become a data scientist.','some random stuff xjksdj skdjfs skdjf']\r\n",
        "\r\n",
        "#Lets start \r\n",
        "tokenizer.fit_on_texts(sentences)  \r\n",
        "word_index = tokenizer.word_index\r\n",
        "print(word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<oov>': 1, 'i': 2, 'am': 3, 'my': 4, 'mahendra': 5, 'kumar': 6, 'currently': 7, 'in': 8, 'second': 9, 'year': 10, 'of': 11, 'study': 12, 'at': 13, 'iit': 14, 'goa': 15, 'branch': 16, 'is': 17, 'maths': 18, 'computing': 19, 'want': 20, 'to': 21, 'become': 22, 'a': 23, 'data': 24, 'scientist': 25, 'some': 26, 'random': 27, 'stuff': 28, 'xjksdj': 29, 'skdjfs': 30, 'skdjf': 31}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9TsLxFISuC-",
        "outputId": "b3129ffa-f639-46cd-b845-28e355901542"
      },
      "source": [
        "#generate sequences corresponding to input \r\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\r\n",
        "print(*sequences,sep='\\n')\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 3, 5, 6]\n",
            "[2, 3, 7, 8, 4, 9, 10, 11, 12, 13, 14, 15]\n",
            "[4, 16, 17, 18, 19]\n",
            "[20, 21, 22, 23, 24, 25]\n",
            "[26, 27, 28, 29, 30, 31]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qLYCmSMUs7W",
        "outputId": "ee256548-ae44-4463-ad1d-39b22414447b"
      },
      "source": [
        "\r\n",
        "#generally we fed data in model having same shapes which is not there so we use padding to achive that \r\n",
        "\r\n",
        "padded_sequences = pad_sequences(sequences)\r\n",
        "print(\"Padding with default settings :\")\r\n",
        "print(*padded_sequences,sep='\\n')\r\n",
        "\r\n",
        "padded_sequences = pad_sequences(sequences,maxlen=6)\r\n",
        "print(\"\\nPadding with maxlen parameter :\")\r\n",
        "print(*padded_sequences,sep='\\n')\r\n",
        "\r\n",
        "padded_sequences = pad_sequences(sequences,maxlen=6,padding='post')\r\n",
        "print(\"\\nPadding with maxlen & post_padding technique :\")\r\n",
        "print(*padded_sequences,sep='\\n')\r\n",
        "\r\n",
        "padded_sequences = pad_sequences(sequences,maxlen=6,padding='post',truncating='post')\r\n",
        "print(\"\\nPadding with above two + truncating settings :\")\r\n",
        "print(*padded_sequences,sep='\\n')\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Padding with default settings :\n",
            "[0 0 0 0 0 0 0 0 2 3 5 6]\n",
            "[ 2  3  7  8  4  9 10 11 12 13 14 15]\n",
            "[ 0  0  0  0  0  0  0  4 16 17 18 19]\n",
            "[ 0  0  0  0  0  0 20 21 22 23 24 25]\n",
            "[ 0  0  0  0  0  0 26 27 28 29 30 31]\n",
            "\n",
            "Padding with maxlen parameter :\n",
            "[0 0 2 3 5 6]\n",
            "[10 11 12 13 14 15]\n",
            "[ 0  4 16 17 18 19]\n",
            "[20 21 22 23 24 25]\n",
            "[26 27 28 29 30 31]\n",
            "\n",
            "Padding with maxlen & post_padding technique :\n",
            "[2 3 5 6 0 0]\n",
            "[10 11 12 13 14 15]\n",
            "[ 4 16 17 18 19  0]\n",
            "[20 21 22 23 24 25]\n",
            "[26 27 28 29 30 31]\n",
            "\n",
            "Padding with above two + truncating settings :\n",
            "[2 3 5 6 0 0]\n",
            "[2 3 7 8 4 9]\n",
            "[ 4 16 17 18 19  0]\n",
            "[20 21 22 23 24 25]\n",
            "[26 27 28 29 30 31]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwInEIofjsCr"
      },
      "source": [
        "# Since Our word_index dictionary contains a lot of stopwords which have no meaning in themselves and hence not required while doing NLP \r\n",
        "\r\n",
        "#Here is list of commonly used stopwords Rest you know how to exclude this words from Sentence using python !! (In case you don't know don't worry I will be doing that in upcoming Real World Example below)\r\n",
        "Stopwords = ['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', 'not', 'as', 'at', 'be', 'because', 'been', 'before', 'being',\\\r\n",
        "             'below', 'between', 'both', 'but', 'by', 'can', 'not', 'cannot', 'could', 'couldn', 'not', 'did', 'didn', 'not', 'do', 'does', 'doesn', 'not', 'doing', 'don', 'not',\\\r\n",
        "             'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', 'not', 'has', 'hasn', 'not', 'have', 'haven', 'not', 'having', 'he', 'he', 'had', 'he', 'will',\\\r\n",
        "             'he', 'is', 'her', 'here', 'here', 'is', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'how', 'is', 'i', 'i', 'had', 'i', 'will', 'i', 'am', 'i', 'have', 'if', 'in', \\\r\n",
        "             'into', 'is', 'isn', 'not', 'it', 'it', 'is', 'its', 'itself', 'let', 'is', 'me', 'more', 'most', 'mustn', 'not', 'my', 'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once',\\\r\n",
        "             'only', 'or', 'other', 'ought', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same', 'shan', 'not', 'she', 'she', 'had', 'she', 'will', 'she', 'is', 'should', 'shouldn',\\\r\n",
        "             'not', 'so', 'some', 'such', 'than', 'that', 'that', 'is', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'there', 'is', 'these', 'they', 'they', 'had', \\\r\n",
        "             'they', 'will', 'they', 'are', 'they', 'have', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', 'wasn', 'not', 'we', 'we', 'had', 'we', 'will',\\\r\n",
        "             'we', 'are', 'we', 'have', 'were', 'weren', 'not', 'what', 'what', 'is', 'when', 'when', 'is', 'where', 'where', 'is', 'which', 'while', 'who', 'who', 'is', 'whom', 'why',\\\r\n",
        "             'why', 'is', 'with', 'won', 'not', 'would', 'wouldn', 'not', 'you', 'you', 'had', 'you', 'will', 'you', 'are', 'you', 'have', 'your', 'yours', 'yourself', 'yourselves', 'MySQL', 'Stopw']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLW_SoSbczeX"
      },
      "source": [
        "# Second Common thing we encounter is that in real life people use shorthands like I've, I'am, we'll etc. etc. and Also there \r\n",
        "#could be  puncutation mark which are of  not use at all . To handle that scenerios , I have written these two functions\r\n",
        "\r\n",
        "#To make shorhand like <I've> to their full form <I have > .These are some that I am familier with \r\n",
        "def clean_sentence(sentence):\r\n",
        "\r\n",
        "  # A little bit sentence cleaning first to handle scenerio like his-her (note if dash is removed then 'hisher' is a nonsense word) by adding comma before and after such symbols . \r\n",
        "  sentence = sentence.replace('-',' - ')\r\n",
        "  sentence = sentence.replace('.',' . ')\r\n",
        "  sentence = sentence.replace('/',' / ')\r\n",
        "  sentence = sentence.replace(',', ' , ')  # and so on.....as much as you like but these are a few common ones\r\n",
        "\r\n",
        "  Words=sentence.split()\r\n",
        "  sentence = ''\r\n",
        "  for word in Words:\r\n",
        "    word = word.replace(\"'s\",' is')\r\n",
        "    word = word.replace(\"'nt\",' not')\r\n",
        "    word = word.replace(\"n't\",' not')\r\n",
        "    word = word.replace(\"'d\", ' had')\r\n",
        "    word = word.replace(\"'ll\",' will')\r\n",
        "    word = word.replace(\"'m\",' am')\r\n",
        "    word = word.replace(\"'ve\",' have')\r\n",
        "    word = word.replace(\"'re\", ' are')\r\n",
        "    sentence = sentence + word + ' '\r\n",
        "  return sentence \r\n",
        "\r\n",
        "\r\n",
        "#To remove the punctuation marks (if any) in sentences\r\n",
        "def remove_punctuations(sentence):\r\n",
        "  import string\r\n",
        "  table = str.maketrans('','',string.punctuation)\r\n",
        "  Words = sentence.split()\r\n",
        "  sentence = ''\r\n",
        "  for word in Words : \r\n",
        "    word = word.translate(table)\r\n",
        "    sentence = sentence + word+ ' '\r\n",
        "  return sentence\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lar2aPKnjowL",
        "outputId": "4432d811-b707-46b0-de67-43c91886205c"
      },
      "source": [
        "## One last scenerio which comes in my mind is web_scrapping. In HTML codes there are syntax like <h>, <div> etc. which are of no use. To remove those there is a something called Beautiful Soap. \r\n",
        "\r\n",
        "#For example :-\r\n",
        "\r\n",
        "html_code = \"\"\"\r\n",
        "            <!DOCTYPE html>\r\n",
        "            <html>\r\n",
        "            <body>\r\n",
        "\r\n",
        "            <h1>My First Heading</h1>\r\n",
        "\r\n",
        "            <p>My first paragraph.</p>\r\n",
        "\r\n",
        "            </body>\r\n",
        "            </html>\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "soup = BeautifulSoup(html_code)\r\n",
        "sentence = soup.get_text()\r\n",
        "print(sentence)\r\n",
        "\r\n",
        "## With this we will now take up a Real World Data and Dirty our hands to make that data clean !! LETS GO---->"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "My First Heading\n",
            "My first paragraph.\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5kXuNEEq2NU"
      },
      "source": [
        "#--------------------------------------------------------------------### A REAL WORLD EXMAPLE ####----------------------------------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDaznlAhquhC"
      },
      "source": [
        "#Here we will use a  dataset available in tensorflow_datasets which is \"IMDb\" (Internet Movies Databases) containing various movies reviews and corresponding sentiment 'Good/Bad'\r\n",
        "# We will only extract words which are present most frequently with some filtering techniques we know now\r\n",
        "#Importing the dataset IMDb \r\n",
        "\r\n",
        "import tensorflow_datasets as tfds \r\n",
        "data = tfds.as_numpy(tfds.load('imdb_reviews',split='train'))\r\n",
        "\r\n",
        "#In this dataset there are 50,000 reviews!!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CkFNH18z-NT"
      },
      "source": [
        "movie_reviews = []\r\n",
        "for item in data : \r\n",
        "  review = str(item['text'].decode('UTF-8').lower())\r\n",
        "  soup = BeautifulSoup(review)\r\n",
        "  review = soup.get_text()\r\n",
        "  review = clean_sentence(review)\r\n",
        "  review = remove_punctuations(review)\r\n",
        "  movie_reviews.append(review)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "qYMLwxt16exb",
        "outputId": "57f60dd9-151e-459e-b904-829440ca576b"
      },
      "source": [
        "movie_reviews[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'this was an absolutely terrible movie  do not be lured in by christopher walken or michael ironside  both are great actors  but this must simply be their worst role in history  even their great acting could not redeem this movie is ridiculous storyline  this movie is an early nineties us propaganda piece  the most pathetic scenes were those when the columbian rebels were making their cases for revolutions  maria conchita alonso appeared phony  and her pseudo  love affair with walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning  i am disappointed that there are movies like this  ruining actor is like christopher walken is good name  i could barely sit through it  '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uZ1EoxxyWXe"
      },
      "source": [
        "# Now do some tokenization and remove the stopwords from these cleaned sentences :>\r\n",
        "\r\n",
        "# removing the stopwords from sentences \r\n",
        "updated = []\r\n",
        "for review in movie_reviews : \r\n",
        "  sentence = ''\r\n",
        "  review = review.split()\r\n",
        "  for word in review : \r\n",
        "    if word not in Stopwords :    #This has been defined initially at the start You can go back  and check if you don't remember \r\n",
        "      sentence = sentence + word + ' '\r\n",
        "  updated.append(sentence)\r\n",
        "movie_reviews = updated \r\n",
        "\r\n",
        "#Tokenization \r\n",
        "tokenizer = Tokenizer(num_words=5000,oov_token='<oov>')\r\n",
        "tokenizer.fit_on_texts(movie_reviews)\r\n",
        "word_index = tokenizer.word_index\r\n",
        "sequences = tokenizer.texts_to_sequences(movie_reviews)  # You can pass some other list of sentences here too to get their tokens. If some word in not present in word_index it will shows <oov>\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "UO45xGVd7QuS",
        "outputId": "fb9176b3-4823-4c24-f803-3d6c8ffe8e44"
      },
      "source": [
        "movie_reviews[0]  ## See yourself how many stopwords were there ....phww "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'absolutely terrible movie lured christopher walken michael ironside great actors must simply worst role history even great acting redeem movie ridiculous storyline movie early nineties us propaganda piece pathetic scenes columbian rebels making cases revolutions maria conchita alonso appeared phony pseudo love affair walken nothing pathetic emotional plug movie devoid real meaning disappointed movies like ruining actor like christopher walken good name barely sit '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgC3QOjR76dd",
        "outputId": "c5686717-acc1-47d2-d0b2-25cb7f580a4c"
      },
      "source": [
        "## Coverting sequences into rectangular matrix \r\n",
        "\r\n",
        "updated_sequences = pad_sequences(sequences,padding='post')\r\n",
        "print('Shape of Matrix which will be fed in model : ',len(updated_sequences[0]),'x',len(updated_sequences))\r\n",
        "\r\n",
        "\r\n",
        "#Note :\r\n",
        "## This model can be futhur imporved by removing the words which have no meaning and are some random text , That we will see later.. Till then bye!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of Matrix which will be fed in model :  1422 x 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToK7mQroAfVQ",
        "outputId": "2b16a403-7b2a-46ae-97ef-7d2a9b41c8a7"
      },
      "source": [
        "print('Some Top Words present in Reviews are :',list(word_index.keys())[1:20])  #IN decreasing order of frequency\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some Top Words present in Reviews are : ['movie', 'film', 'one', 'like', 'just', 'good', 'time', 'even', 'story', 'really', 'see', 'well', 'much', 'get', 'bad', 'people', 'also', 'great', 'first']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLEdcTJc-pyl"
      },
      "source": [
        "# Decoder can be used convert these sequences back to review given except the text which was not recognized\r\n",
        "Decoder = {}\r\n",
        "for key,value in word_index.items():\r\n",
        "  Decoder[value]=key\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8JI-ej89cmr",
        "outputId": "a5c87eb1-c3b2-45c5-c7c4-63217efc1159"
      },
      "source": [
        "## Lets give a review and see its sequence :\r\n",
        "review = 'I saw that movie yesterday and I find that very boring still I find other way which is NLP very interesting lets solve some problems in that'\r\n",
        "seq = tokenizer.texts_to_sequences([review])\r\n",
        "print(seq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 112, 1, 2, 3674, 1, 1, 71, 1, 1, 243, 46, 1, 71, 1, 22, 1, 1, 1, 1, 114, 1476, 3195, 1, 574, 1, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d55_s5h_Cs8",
        "outputId": "ddbf9ef4-f1e1-410a-ceb2-af094943c187"
      },
      "source": [
        "decoded_sentence = ''\r\n",
        "for x in seq[0] : \r\n",
        "  if x != 1:\r\n",
        "    decoded_sentence = decoded_sentence + Decoder[x]+ ' '\r\n",
        "print(decoded_sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "saw movie yesterday find boring still find way interesting lets solve problems \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Y3GK9m1_J_d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITtFAgCKDyqI"
      },
      "source": [
        "### Reading JSON files and CSV files in python a"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "fRKApZ2ID6CT",
        "outputId": "92080ea1-9edc-4e02-85d8-dee0d5a1942e"
      },
      "source": [
        "#you can read csv files using pandas read_csv method . Simple isn't it? \r\n",
        "import pandas as pd \r\n",
        "data = pd.read_csv(r'ufo.csv')\r\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>City</th>\n",
              "      <th>Colors Reported</th>\n",
              "      <th>Shape Reported</th>\n",
              "      <th>State</th>\n",
              "      <th>Time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ithaca</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TRIANGLE</td>\n",
              "      <td>NY</td>\n",
              "      <td>6/1/1930 22:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Willingboro</td>\n",
              "      <td>NaN</td>\n",
              "      <td>OTHER</td>\n",
              "      <td>NJ</td>\n",
              "      <td>6/30/1930 20:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Holyoke</td>\n",
              "      <td>NaN</td>\n",
              "      <td>OVAL</td>\n",
              "      <td>CO</td>\n",
              "      <td>2/15/1931 14:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Abilene</td>\n",
              "      <td>NaN</td>\n",
              "      <td>DISK</td>\n",
              "      <td>KS</td>\n",
              "      <td>6/1/1931 13:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>New York Worlds Fair</td>\n",
              "      <td>NaN</td>\n",
              "      <td>LIGHT</td>\n",
              "      <td>NY</td>\n",
              "      <td>4/18/1933 19:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   City Colors Reported Shape Reported State             Time\n",
              "0                Ithaca             NaN       TRIANGLE    NY   6/1/1930 22:00\n",
              "1           Willingboro             NaN          OTHER    NJ  6/30/1930 20:00\n",
              "2               Holyoke             NaN           OVAL    CO  2/15/1931 14:00\n",
              "3               Abilene             NaN           DISK    KS   6/1/1931 13:00\n",
              "4  New York Worlds Fair             NaN          LIGHT    NY  4/18/1933 19:00"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50eTYZ-aIfTm",
        "outputId": "117e6c28-d332-4f39-b815-60a90feeee1c"
      },
      "source": [
        "#Now if you want any column as training column lets say City column here then just select it like this\r\n",
        "\r\n",
        "train = data['City'].tolist()\r\n",
        "print(train[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Ithaca', 'Willingboro', 'Holyoke', 'Abilene', 'New York Worlds Fair', 'Valley City', 'Crater Lake', 'Alma', 'Eklutna', 'Hubbard']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFTVAMrWITUS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "217768a2-f16a-4fd0-c28c-1f5d315840de"
      },
      "source": [
        "#loading json file \r\n",
        "!wget --no-check-certificate \\\r\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json \\\r\n",
        "    -O /tmp/sarcasm.json\r\n",
        "\r\n",
        "#Now we will see how to import JSON Files . pandas currently do not load JSON files directly . \r\n",
        "# for that we will use json library in python and make our life easier\r\n",
        "\r\n",
        "import json\r\n",
        "\r\n",
        "with open('/tmp/sarcasm.json','r') as f:\r\n",
        "  data = json.load(f)\r\n",
        "\r\n",
        "data[:5]\r\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-25 21:41:50--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.97.128, 74.125.134.128, 74.125.141.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.97.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5643545 (5.4M) [application/json]\n",
            "Saving to: ‘/tmp/sarcasm.json’\n",
            "\n",
            "\r/tmp/sarcasm.json     0%[                    ]       0  --.-KB/s               \r/tmp/sarcasm.json   100%[===================>]   5.38M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-01-25 21:41:50 (165 MB/s) - ‘/tmp/sarcasm.json’ saved [5643545/5643545]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'article_link': 'https://www.huffingtonpost.com/entry/versace-black-code_us_5861fbefe4b0de3a08f600d5',\n",
              "  'headline': \"former versace store clerk sues over secret 'black code' for minority shoppers\",\n",
              "  'is_sarcastic': 0},\n",
              " {'article_link': 'https://www.huffingtonpost.com/entry/roseanne-revival-review_us_5ab3a497e4b054d118e04365',\n",
              "  'headline': \"the 'roseanne' revival catches up to our thorny political mood, for better and worse\",\n",
              "  'is_sarcastic': 0},\n",
              " {'article_link': 'https://local.theonion.com/mom-starting-to-fear-son-s-web-series-closest-thing-she-1819576697',\n",
              "  'headline': \"mom starting to fear son's web series closest thing she will have to grandchild\",\n",
              "  'is_sarcastic': 1},\n",
              " {'article_link': 'https://politics.theonion.com/boehner-just-wants-wife-to-listen-not-come-up-with-alt-1819574302',\n",
              "  'headline': 'boehner just wants wife to listen, not come up with alternative debt-reduction ideas',\n",
              "  'is_sarcastic': 1},\n",
              " {'article_link': 'https://www.huffingtonpost.com/entry/jk-rowling-wishes-snape-happy-birthday_us_569117c4e4b0cad15e64fdcb',\n",
              "  'headline': 'j.k. rowling wishes snape happy birthday in the most magical way',\n",
              "  'is_sarcastic': 0}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijOzxG0qfLVy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}